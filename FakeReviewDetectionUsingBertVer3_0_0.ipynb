{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "FakeReviewDetectionUsingBertVer3-0-0.ipynb",
      "mount_file_id": "1nS1Vxxy7yra-z2H436oHl7KMAxT_QAU6",
      "authorship_tag": "ABX9TyM6caD6aYn8hDeY400eP6+z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ras1364/Fake-Review-Using-BERT/blob/main/FakeReviewDetectionUsingBertVer3_0_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1PdDVhRexpxq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19d13c7a-5e15-4fce-ce5b-7e801d94b67a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.26.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.utils import np_utils\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from string import ascii_lowercase\n",
        "from collections import Counter\n",
        "import itertools, nltk, snowballstemmer, re\n",
        "\n",
        "!pip install transformers\n",
        "import transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = pd.read_csv(\"/content/sample_data/X_test.csv\")\n",
        "X_train = pd.read_csv(\"/content/sample_data/X_train.csv\")\n",
        "y_test = pd.read_csv(\"/content/sample_data/y_test.csv\")\n",
        "y_train = pd.read_csv(\"/content/sample_data/y_train.csv\")"
      ],
      "metadata": {
        "id": "L3jbVEROyLEC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 100\n",
        "# for x in range(0,len(X_train)):\n",
        "#   line = X_train.loc[x].at[\"stemmed_text_data\"]\n",
        "#   if max_length < len(line.split()) :\n",
        "#     max_length = len(line.split())\n",
        "\n",
        "print(max_length)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_IU66B2rG_e",
        "outputId": "89a5ecb1-83b8-48de-c191-0d79bdff499f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "150\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, TFBertModel \n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
        "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n"
      ],
      "metadata": {
        "id": "oXSRHOvQ__xe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5932706-0b28-4207-dd51-f0100408193f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(text, maxlen):\n",
        "  input_ids=[]\n",
        "  attention_masks=[]\n",
        "\n",
        "  for row in text:\n",
        "    encoded = tokenizer.encode_plus(\n",
        "        row,\n",
        "        add_special_tokens=True,\n",
        "        max_length=maxlen,\n",
        "        pad_to_max_length=True,\n",
        "        return_attention_mask=True,\n",
        "    )\n",
        "    input_ids.append(encoded['input_ids'])\n",
        "    attention_masks.append(encoded['attention_mask'])\n",
        "\n",
        "  return np.array(input_ids),np.array(attention_masks)"
      ],
      "metadata": {
        "id": "vhBWHH8iALjh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X_train_encoded = encode(X_train[\"stemmed_text_data\"].tolist(), maxlen=max_length)\n",
        "# X_test_encoded = encode(X_test[\"stemmed_text_data\"].tolist(), maxlen=max_length)\n",
        "X_train_input_ids, X_train_attention_masks = encode(X_train[\"stemmed_text_data\"].tolist(), maxlen=max_length)\n",
        "X_test_input_ids, X_test_attention_masks = encode(X_test[\"stemmed_text_data\"].tolist(), maxlen=max_length)"
      ],
      "metadata": {
        "id": "Odn8HYroAPmH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c270d771-4169-4ca8-84dc-891f548cf997"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def build_model(bert_model):\n",
        "   input_word_ids = tf.keras.Input(shape=(max_length,),dtype='int32')\n",
        "   attention_masks = tf.keras.Input(shape=(max_length,),dtype='int32')\n",
        "\n",
        "   sequence_output = bert_model([input_word_ids,attention_masks])\n",
        "   output = sequence_output[1]\n",
        "   output = tf.keras.layers.Dense(32,activation='relu')(output)\n",
        "   output = tf.keras.layers.Dropout(0.2)(output)\n",
        "   output = tf.keras.layers.Dense(1,activation='sigmoid')(output)\n",
        "\n",
        "   model = tf.keras.models.Model(inputs = [input_word_ids,attention_masks], outputs = output)\n",
        "   model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "   return model"
      ],
      "metadata": {
        "id": "OMgQ8Bi6eD9D"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(bert_model)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_B3UyR4eHEe",
        "outputId": "5bac27c3-3237-42c5-d98c-8b290f9f2802"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 150)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 150)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_1[0][0]',                \n",
            "                                thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 150,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 32)           24608       ['tf_bert_model[0][1]']          \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 32)           0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 1)            33          ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,506,881\n",
            "Trainable params: 109,506,881\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    [X_train_input_ids, X_train_attention_masks],\n",
        "    y_train,\n",
        "    batch_size=32,\n",
        "    epochs=5,\n",
        "    validation_data=([X_test_input_ids, X_test_attention_masks], y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LfRCHqVJgKIa",
        "outputId": "f68d87fa-30d0-4e03-9f8c-755d52ec2dcb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "35/35 [==============================] - 2066s 58s/step - loss: 0.7010 - accuracy: 0.5080 - val_loss: 0.6901 - val_accuracy: 0.5063\n",
            "Epoch 2/5\n",
            "35/35 [==============================] - 1976s 57s/step - loss: 0.6863 - accuracy: 0.5473 - val_loss: 0.6794 - val_accuracy: 0.6021\n",
            "Epoch 3/5\n",
            "35/35 [==============================] - 1979s 57s/step - loss: 0.6668 - accuracy: 0.5884 - val_loss: 0.6002 - val_accuracy: 0.6812\n",
            "Epoch 4/5\n",
            "35/35 [==============================] - 1985s 57s/step - loss: 0.5392 - accuracy: 0.7330 - val_loss: 0.5048 - val_accuracy: 0.7542\n",
            "Epoch 5/5\n",
            "35/35 [==============================] - 1982s 57s/step - loss: 0.4337 - accuracy: 0.8250 - val_loss: 0.4719 - val_accuracy: 0.7875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate([X_test_input_ids, X_test_attention_masks], y_test)\n",
        "print('Test accuracy :', accuracy ,' and loss = ', loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dj2aogEM5Al3",
        "outputId": "c719ce94-c8d7-4fbc-a4ad-183cd610e284"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15/15 [==============================] - 253s 17s/step - loss: 0.4719 - accuracy: 0.7875\n",
            "Test accuracy : 0.7875000238418579  and loss =  0.4719296395778656\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_predicted = model.predict([X_test_input_ids, X_test_attention_masks])\n",
        "y_predicted = y_predicted.flatten()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9mH40-h5Bd3",
        "outputId": "c3cc81ef-493c-4bc5-fe32-3c3e939d5cc7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15/15 [==============================] - 269s 18s/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "y_predicted = np.where(y_predicted > 0.5, 1, 0)\n",
        "y_predicted"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgCaVN7egA03",
        "outputId": "77c2b4ce-f8eb-486f-b235-679a208dffa7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n",
              "       1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1,\n",
              "       1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,\n",
              "       0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,\n",
              "       1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,\n",
              "       0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,\n",
              "       0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,\n",
              "       1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,\n",
              "       0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
              "       1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
              "       1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,\n",
              "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,\n",
              "       0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,\n",
              "       1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,\n",
              "       0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
              "       1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,\n",
              "       0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,\n",
              "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,\n",
              "       1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,\n",
              "       0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,\n",
              "       0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "cm = confusion_matrix(y_test, y_predicted)\n",
        "cm "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6n4dB2BeiRj",
        "outputId": "77bbd0a6-3a2c-45dc-97bd-87b4eec450f7"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[207,  33],\n",
              "       [ 69, 171]])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sn\n",
        "sn.heatmap(cm, annot=True, fmt='d')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Truth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "Z-aCw0JtekH_",
        "outputId": "bd326bf2-95b8-42da-b8b7-dd2a07ca6fe1"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(33.0, 0.5, 'Truth')"
            ]
          },
          "metadata": {},
          "execution_count": 24
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test, y_predicted))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDYSfqCviGAC",
        "outputId": "12f13f2e-5784-46d0-e947-82be1d5efff3"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.86      0.80       240\n",
            "           1       0.84      0.71      0.77       240\n",
            "\n",
            "    accuracy                           0.79       480\n",
            "   macro avg       0.79      0.79      0.79       480\n",
            "weighted avg       0.79      0.79      0.79       480\n",
            "\n"
          ]
        }
      ]
    }
  ]
}